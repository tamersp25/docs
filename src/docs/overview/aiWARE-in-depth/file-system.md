<!-- markdownlint-disable -->

Overview
========

In New Edge, a filesystem is used as the I/O mechanism for Engines to read data and write results. It is also used as an execution-time cache if one is required by the Engine. For each discrete Engine process, all of the information contained in the DB and additional debug data will be persisted in the filesystem. Being the single source of persisted I/O data used by Engines and Controller, the File System will support both the planned execution path (DAG) and the various system error cases. It will contain enough information to rebuild the database for reprocessing or restarting tasks and jobs. The file system is NFS 4 and mounted through a docker NFS deployment.

## Folder Structure

```pre
/cache
    /mapping.txt
    /<#> of cache instance
        /last two digits of org
            /orgId
                /job
                    /last 2 digits of job in lowercase
                        /job id
                            /definition.json (optional)
                            /status.json
                            /task
                                /taskid
                                    /io (adapters, chunk)
                                        /io-id
                                            /status.json
                                            /000000/ (thousends)
                                                001.data
                                                001.ctrl
                                                001.info (optional)
                                    /output.engine_id.log
                                    /error.engine_id.log
                                    /task_payload.json
                                    /metadata.json
                                    /status.json
                                /assets (SI Output)
                                    /playback
                                    /processing
                                /original
        /_failed_job
            /last 2 digits of job in lowercase
                /job id
                    /jobs and their children, that have failed tasks are copied here
        /source
            /<source_id> -- transient can be gone
        /engine
            /<engineid>
            /instance
                /<engine_instance_id>
        /library
```

Cache Folder
------------

This is the space used for Engines to read and write I/O data as part of the Job DAG execution. The Controller ultimately defines the folder tree all the way from the root cache folder to the IO folder based on the Job DAG. Engine Toolkit is the one reading and writing from this space on behalf of the Engines in order to get the data needed for executing a Task and to write results so they can be consumed in following Tasks.

The cache folder is generally organized by Org -> Job -> Task -> IO. Each IO folder contains chunks of data being read or written by an Engine. For each chunk of data, two files are saved to the IO folder: .data & .ctrl. In order to meet File System best practices, large folders like the ones containing Orgs of Jobs are subdivided into multiple directories by the last two digits of the ID. In the same way, IO folders are subdivided by the thousands digit.

Each Task in the DAG is associated with three sets of file system folders: Input folders from which the Task gets its data to process, Output folders for chunks generated by the Task, and Child Input folders to be used by the next Task on the DAG.

In order to process a Job DAG, each Task requires 0-N inputs, either from previous Tasks or from an external source. Therefore, a Task can have more than one IO Input folders. Each IO Output can have 0-N Child Input folders associated with it. Similarly, each Task can have 0-N Output folders which may be required if a Task wants to generate multiple different data sets, e.g., frames and audio files.

As each output file is created, a hardlink to this file is created in the Child Input folder for the next set of tasks in the DAG.

From an implementation perspective, an IO object [add link] represents one output or one input folder in the File System. We use the same IO object for both input and output folders as this designation depends on the point of view of the consumer, however, the files in the folder will have the respective extension - .IN or .OUT. [see question above].

More on how Engine Toolkit and Controller are using the File System to implement the DAG can be found in [this link].

![](https://docs.google.com/drawings/u/0/d/szOqfHDDww5DKnJLxY13U-w/image?w=575&h=274&rev=1&ac=1&parent=1OSsYFZsmAG28Y83pyV2GS-wNicw_qPkkavJrlaLFYWk)

### File Naming Conventions

As Engines read and write from the respective I/O folders, the following naming convention should be followed:

[Index level 0] . [Index level N] _ [Timestamp] _ [Parent ID] . [IO Type] . [Base] . [Modifier] . [Attempt Count]

Item | Description
-- | --
\[Index level N] | This is a numeric counter starting at 001 and increases with each output chunk or stream chunk. A consumer can use multiple of those fields, separated by '.', to construct the index of its output in a tiered way, e.g., 001.001_\[rest of the filename]
\[Timestamp] | (Only present at non-stream files). The time in which the chunk is produced, as provided by the Engine, or if not provided the default is the current time. 
\[Parent ID] | Each Engine generates a GUID for its instance when it starts and registers with Controller. This GUID is used for all files generated by this engine instance.
\[IO Type] | There are two IO types, 'out' and 'in'. When an engine produces a chunk it always generates a chunk with IO type 'out' inside an IO object for output. If any child IO objects exist, chunks of type 'in' are also generated with hard links to the files of 'out' chunks.
\[Base] | 'data' for the contents of a  chunk, 'ctrl' for the control file, and 'info' for any logging particular to that chunk.
\[Modifier] | (Optional) Only applies to chunks of IO type "in". When missing, the chunk is always available. When 'P' the chunk is being processed, when 'DONE' processing has finished and when 'ERROR' it can no longer be processed due to errors.
\[Attempt Count] | (Optional) Only applies to chunks of IO type 'in' and with a modifier present. It specifies the current or last attempt that was made to process the chunk starting with 0


### Roles and definitions of the files in the system

Item | Description
-- | --
mapping.txt / cache.json | \[???]
/\[job_id]/definition.json | The original Job definition as provided by Core. Schema can be found \[add link].
/\[job_id]/status.json | NOT IMPLEMENTED
/\[task_id]/status.json | The status of the task, which is being set by the Controller \[???] Statuses: "not_defined", "pending", "running", "complete", "failed"
/\[io_id]/status.json | The status of the task, which is being set by the Engine Toolkit Statuses: "not_defined", "pending", "running", "complete", "failed"
\[io_id]/00000/001.01_[timestamp]_[parentId].out.data | The data file of a chunk with index 001.01, a timestamp, and a parent Id.
\[io_id]/00000/001.01_\[timestamp]_\[parentId].out.ctrl.DONE.3 | The control file of a chunk with index 001.01, a timestamp, and a parent id. The extension indicates the chunk has been processed successfully after 3 attempts. This is a JSON file which contains the CRC32 checksum of the data file and user metadata, if any, provided when the chunk was finalized.
\[task_id]/output.engine_id.log | [???]
\[task_id]/error.engine_id.log | \[???]
\[task_id]/task_payload.json | The Task payload coming directly from Core.
\[task_id]/metadata.json | [???]


Failures Folder
---------------

#### [TODO]

Source Folder
-------------

#### [TODO]

Engines Folder
--------------

#### [TODO]

Libraries Folder
----------------

#### [TODO]

Implementation
==============

The root folder for the File System auto-generated Go docs: <http://localhost:6060/pkg/veritone.com/realtime/modules/scfs/>

[TODO: Find a way to render the auto-docs directly into this doc, here or in a web version of it]

Interfaces Related to Job Processing
------------------------------------

### Cache

<http://localhost:6060/pkg/veritone.com/realtime/modules/scfs/#Cache>

### Org

<http://localhost:6060/pkg/veritone.com/realtime/modules/scfs/#Org>

### Job

<http://localhost:6060/pkg/veritone.com/realtime/modules/scfs/#Job>

### Task

<http://localhost:6060/pkg/veritone.com/realtime/modules/scfs/#Task>

### IO

<http://localhost:6060/pkg/veritone.com/realtime/modules/scfs/#IO>

### Chunk

<http://localhost:6060/pkg/veritone.com/realtime/modules/scfs/#Chunk>

### Chunk Name

<http://localhost:6060/pkg/veritone.com/realtime/modules/scfs/#ChunkFileName>

Open Questions
==============

1.  Reasons for having multiple cache folders? - Sharding for scaling I/O, separating org content into a separate silo

2.  Are we tied to EFS? No, stage & prod will use nfs.

3.  The schema for JSON files --- We should define better.

4.  What are 'assets' and 'original' under the /task folder? We need clarity on this.  The idea was to keep all assets and original files so we can reproduce and trace the operations.  Assets was for easier lookup from other tasks.

5.  Who sets statuses? And why are Task and IO share the status const? Engine/Engine Toolkit -> Controller; Controller Primary changes from scheduled -> pending

6.  What are the library, source and engine folders used for, and how exactly? Source - scratch space for podcast/rss, youtube for enhanced processing --- it is not persistent so the results need to go back into taskOutput; library --- we need to define (spec) how libraries get downloaded and leveraged inside v3.  Same for extending libraries. Engine --- This is scratch space for the engine to store downloaded assets such as tensorflow models

7.  Why IO files are not separated to Input and Outputs? 

8.  Reconsider the use of Timestamp. Timestamp is optional and used for debug/tracing purposes.

9.  How is the FS used for Engine execution cache?

10. Where is the Engine log and how can it be consumed (per Engine, Task, Job, etc.) 

1.  This still needs to be implemented.  The log is written to ~task/error.<engine_instance>.log and ~task/output..<engine_instance>.log by engine toolkit.  

2.  Christos & Quynh should implement