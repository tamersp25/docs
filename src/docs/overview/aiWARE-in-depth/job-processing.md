<!-- markdownlint-disable -->

# Job Processing

## Overview

Fundamental to the value of Veritone aiWARE Edge is the ability to ingest and process data with a set of Engines and in an order defined by the user. A single end-to-end processing of a workflow defined by a user is called a Job. Every Job is made out of 1-N Tasks, each Task is implemented by 1-N instances of a specific type of Engine. This document describes how a Job is processed by Edge 3.0.

### Job Processing Flow

Each Job can be represented by a Directed Acyclic Graph (DAG) that defines the path that data will flow from ingestion to final Engine execution. Each node on the graph is a Task and represents an Engine (or Adapter) on Edge. Each output of one engine can become the input of another.

For the most part, each Job is associated with a static DAG, defined apriori to execution on Edge; however, the architecture and Edge API's support dynamic DAGs (modified during runtime, while the data is being routed through the DAG).

A route is defined by a JSON payload for serialization and communication between services. In the Edge DB, in addition to storing the JSON, there is a set of routing tables, that break the routes down into more granular parts for tracking and process management. The relevant DB tables are [ADD LINKS] Task, Task_Vertix, & DAG_IO. The Edge subsystem in charge of Job processing is the [Controller](https://docs.google.com/document/d/19VBinwGq-j1xOvVqyej2o-1K3E_QmhNPvqvtahwgeVU/edit#heading=h.y8mqjoka3z7u).

In order to process a Job DAG, each Task requires 0-N inputs, either from previous Tasks or from an external source. All the data needed for executing a Job is kept in the [File System](https://docs.google.com/document/d/1OSsYFZsmAG28Y83pyV2GS-wNicw_qPkkavJrlaLFYWk/edit#). Each Task in the DAG is associated with three sets of file system folders: Input folders from which the Task gets its data to process, Output folders for chunks generated by the Task, and Child Input folders to be used by the next Task on the DAG.

![](https://docs.google.com/drawings/u/0/d/sAhNmw41aJqoq6WXcrhRlcw/image?w=575&h=274&rev=1&ac=1&parent=1CdxuBz01B0eUJU8nKrZLWAF6hKwqJJWh4T6FW3ddPTk)

### DAG Constraints

Stream -> chunk, chunk -> stream, and stream -> stream processing has to be serial, i.e. done by a single engine instance. Chunk -> chunk processing can be serial or parallel, i.e. done by multiple instances of the same Engine. Adapters by definition ingest chunks or streams from external sources, therefore they may run as root nodes or as child nodes, while engines may never run as root nodes of a DAG (except for batch engines).

## Task Processing Flow

1. Engine Toolkit — Scan the Input Folder for up to X items with .IN or .P (skip .ERROR and .DONE). X is passed in by Controller.  
    1. If a file has a .P modifier — meaning someone is being processed by another instance 
        1. If ‘modified’ time is more than the configured length of time of processing — meaning that the current attempt to process has failed. The default time is 90s. (NOTE: Engines should ‘touch’ a file at a heartbeat interval, so that this particular timestep is not dependent on the length of time an engine takes to do its work.) 
            1. Rename back to .IN. Keep # as the number of retries, this number will increment during the processing step as usual. 
                1. If the rename fails, drop the file from your internal memory list as a different engine instance has already renamed it. 

            2. If the # is greater than the max retries, rename the file to .ERROR 
                1. If rename succeeded, update Controller that this message/chunk errored out, and drop it from the memory list. 
                2. If rename fails, then drop the file from your memory list as another engine has done it. 

    2. If ‘parallel’ mode, shuffle the X items in the modified list from step 1. 

2. Engine Toolkit — Select the top file from the list and send the chunk to the engine managed by the engine toolkit. If no items on the list, go to Step 1. 
    1. Rename from .IN.# to .P.[#+1] 
        1. If rename fails, skip file, drop from work list. 
            1. If the operation fails due to source file not present, then another engine started processing the file.   
            2. If another reason (infrastructure issue, etc) , the file is still skipped and will be picked on another scan of the Input Folder. 

    2. During heartbeats & status updates, ‘touch’ the .P file to modify the timestamp. 

3. Engine — Process the chunk. 
    1. Success 
        1. Engine sends output to Engine Toolkit via Engine Toolkit API over HTTP 

    2. Error 
        1. Engine sends back error on processing 

    3. Engine is no longer responsive 
        1. Engine Toolkit fails, notifies controller, renames from .P.# to .IN.[#+1], and terminates itself 

4. Engine Toolkit —   
    1. If successful and if there is output 
        1. Use the file naming conventions, [here](https://docs.google.com/document/d/1OSsYFZsmAG28Y83pyV2GS-wNicw_qPkkavJrlaLFYWk/edit#).  
        2. Write the .OUT.TMP file to Output Folder (ctrl? Is TMP valid?) 
        3. Write the .DATA file 
        4. Rename .OUT.TMP to .OUT 
        5. Hard link .OUT to the Child Input Folder(s) as .IN 
        6. Hard link .DATA to the Child Input Folder(s) 
        7. Rename .P (or .P.#) to .DONE 

    2. If error 
        1. Rename .P.# to .ERROR 
        2. Write .ERROR.DATA, include Error Code, Reason & Detail 
        3. If cumulative errors is greater than N passed in by Controller. [Note: N has to be higher than the max retry count for the .P’s, as we are trusting toolkit, not the underlying engine and suspect errors are being caused by the underlying engine behaving poorly] 
            1. Move all .ERRORs as reported by InternalEngine to .IN.# so they can be retried (but only if .# is less than max retries) 
            2. Notify Controller of too many errors reached: Error count &gt; N 
            3. Controller tells engine to die and marks DB that engine instance was killed for error &gt; N reason. Toolkit may or may not comply with request from Controller.  
                1. Toolkit fails to die 
                    When Server Agent [ADD LINK] checks in and the instance of the bad engine is still alive, controller responds and tells Server Agent to kill the container and to relaunch a new one. 

                2. Toolkit dies properly 
                    When Server Agent checks in, Controller valiates that engine died and instructs Server Agent to start another process. 

            4. Once the cumulative errors on a specific input directory on the Job (as a % or #), then the Task is failed 
                At the DAG level, meta-data will tell Controller what to do when this happens. For example:  Stop all processing for this DAG, continue processing, notify application, etc. 

5. Engine Toolkit —  
    1. If there are additional items still on the list — Go to step 2. 
    2. If no more items on the list — Go to step 1.


## Engine Errors


Job processing may fail in one of the following scenarios. For the complete list of error codes, see here [ADD CODE LINK].

1. Internal Engine is failing 
    1. Due to corrupt file 
    2. Engine is corrupt and generates errors with good files 
    3. Container crashes e.g., out of memory, or yanked by the infra 

2. Engine Toolkit is failing 
    1. Error reported by the internal Engine 
    2. Internal Engine times out 
    3. Internal Engine dies gracefully or non-gracefully 
    4. Container crashes e.g., out of memory, or yanked by the infra 

3. Controller is failing 
    1. Lost connection to DB 
    2. Internal process crashes 
    3. Container crashes e.g., out of memory, or yanked by the infra

### Engine Recovery

When an engine crashes for whatever reason, the Controller can run another engine in a Recovery Mode. The Recovery Mode field is passed from the Controller to the Engine Toolkit, along one of the following flags:

1. “start at folder head” (default for stream engines) 
    1. Serial engines must start reprocessing from the first input file. 
    2. The parent engine may even still be running and generating new output files and hard links into the Child Input Folder.  
    3. The input directory will have a combination of .IN, .P, .ERROR, and .DONE files. 
    4. In Retry Mode, the first step of the engine is to reset all input files to .IN. 
    5. Engine starts processing Input Folder again. 
    6. Engine Toolkit generates the output file, but before writing to the File System, it checks the Output Folder to see if there is a matching index. 
        1. If true, check input for hardlink file. 
        2. If hardlink true, drop output file on floor, so it doesn’t get duplicated by downstream engines. 
        3. If false, Engine toolkit writes output to Output and Child Input folders. 

2. “recover at error” (default for chunk engines)
    1. All .DONE files are ignored. 
    2. .IN, .P, and .ERROR files are processed by the chunk engine. 
    3. To avoid duplicates, any .P and .ERROR chunks created are first checked for duplicates in the Output and Child Input folders. 
    4. Engine Toolkit generates the output file, but before writing to the File System, it checks the Output Folder to see if there is a matching index. 
        1. If true, check input for hardlink file. 
        2. If hardlink true, drop output file on floor, so it doesn’t get duplicated by downstream engines. 
        3. If false, Engine toolkit writes output to Output and Child Input folders. 

    5. Mark .P file as .DONE in Input folder 

3. “start at latest content”  
    1. This use case is for a real-time streaming scenario where keeping current with new data is more important than processing old content. 
    2. When launched in recovery mode, the Engine ignores all the files in the Input Folder and starts reading from the latest input file index and continue forward.